# -*- coding: utf-8 -*-
"""Emotiona analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MrGLjB8mgzV1x4tJqraiY1jvv4V6_9xi
"""



import zipfile

with zipfile.ZipFile("train.zip", 'r') as zip_ref:
    zip_ref.extractall("train")

import tensorflow
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.losses import categorical_crossentropy
import matplotlib.pyplot as plt
# Working with pre trained model

base_model = MobileNet( input_shape=(224,224,3), include_top= False )

for layer in base_model.layers:
  layer.trainable = False


x = Flatten()(base_model.output)
x = Dense(units=7 , activation='softmax' )(x)

# creating our model.
# creating our model.
model = Model(base_model.input, x)
# Change loss function to sparse_categorical_crossentropy
model.compile(optimizer='adam', loss=tensorflow.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])
train_datagen = ImageDataGenerator(
     zoom_range = 0.2,
     shear_range = 0.2,
     horizontal_flip=True,
     rescale = 1./255
)


train_data = train_datagen.flow_from_directory(directory= "train",
                                               target_size=(224,224),
                                               batch_size=32,
                                               class_mode='sparse') # Change class_mode to sparse


val_data = val_datagen.flow_from_directory(directory= "train",
                                           target_size=(224,224),
                                           batch_size=32,
                                           class_mode='sparse') # Change class_mode to sparse
train_data.class_indices
val_datagen = ImageDataGenerator(rescale = 1./255 )


t_img, label = next(train_data)

def plotImages(img_arr, label):
    """
    input  :- images array
    output :- plots the images
    """
    count = 0
    for im, l in zip(img_arr, label):
        plt.imshow(im.astype('uint8'))  # To avoid weird color shifts
        plt.title(f"Shape: {im.shape}")
        plt.axis('off')
        plt.show()

        count += 1
        if count == 10:
            break

plotImages(t_img, label)
from keras.callbacks import ModelCheckpoint, EarlyStopping

# early stopping
es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')

# model check point
mc = ModelCheckpoint(filepath="best_model.h5", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')

# puting call back in a list
call_back = [es, mc]
hist = model.fit(
    train_data,
    steps_per_epoch=10,
    epochs=30,
    validation_data=val_data,
    validation_steps=8,
    callbacks=[es, mc]
)
from keras.models import load_model
model = load_model("/content/best_model.h5")
h =  hist.history
h.keys()

import tensorflow
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.losses import categorical_crossentropy
import matplotlib.pyplot as plt
# Working with pre trained model

base_model = MobileNet( input_shape=(224,224,3), include_top= False )

for layer in base_model.layers:
  layer.trainable = False


x = Flatten()(base_model.output)
x = Dense(units=7 , activation='softmax' )(x)

# creating our model.
# creating our model.
model = Model(base_model.input, x)
# Change loss function to sparse_categorical_crossentropy
model.compile(optimizer='adam', loss=tensorflow.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])
train_datagen = ImageDataGenerator(
     zoom_range = 0.2,
     shear_range = 0.2,
     horizontal_flip=True,
     rescale = 1./255
)


train_data = train_datagen.flow_from_directory(directory= "train",
                                               target_size=(224,224),
                                               batch_size=32,
                                               class_mode='sparse') # Change class_mode to sparse

# Define val_datagen before using it
val_datagen = ImageDataGenerator(rescale = 1./255 )

val_data = val_datagen.flow_from_directory(directory= "train",
                                           target_size=(224,224),
                                           batch_size=32,
                                           class_mode='sparse') # Change class_mode to sparse
train_data.class_indices



t_img, label = next(train_data)

def plotImages(img_arr, label):
    """
    input  :- images array
    output :- plots the images
    """
    count = 0
    for im, l in zip(img_arr, label):
        plt.imshow(im.astype('uint8'))  # To avoid weird color shifts
        plt.title(f"Shape: {im.shape}")
        plt.axis('off')
        plt.show()

        count += 1
        if count == 10:
            break

plotImages(t_img, label)
from keras.callbacks import ModelCheckpoint, EarlyStopping

# early stopping
es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')

# model check point
mc = ModelCheckpoint(filepath="best_model.h5", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')

# puting call back in a list
call_back = [es, mc]
hist = model.fit(
    train_data,
    steps_per_epoch=10,
    epochs=30,
    validation_data=val_data,
    validation_steps=8,
    callbacks=[es, mc]
)
from keras.models import load_model
model = load_model("/content/best_model.h5")
h =  hist.history
h.keys()

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.mobilenet import MobileNet
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt

# Load base MobileNet model
base_model = MobileNet(input_shape=(224, 224, 3), include_top=False)

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom classification head
x = Flatten()(base_model.output)
x = Dense(units=7, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=x)

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.sparse_categorical_crossentropy,
              metrics=['accuracy'])

# Data generators
train_datagen = ImageDataGenerator(
    zoom_range=0.2,
    shear_range=0.2,
    horizontal_flip=True,
    rescale=1./255
)

val_datagen = ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(
    directory="train",
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse'
)

val_data = val_datagen.flow_from_directory(
    directory="train",
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse'
)

# Plot sample images
t_img, label = next(train_data)
def plotImages(img_arr, label):
    for count, (im, l) in enumerate(zip(img_arr, label)):
        plt.imshow(im.astype('uint8'))
        plt.title(f"Shape: {im.shape}")
        plt.axis('off')
        plt.show()
        if count == 9: break

plotImages(t_img, label)

# Callbacks
es = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=5, verbose=1, mode='auto')
mc = ModelCheckpoint(filepath="best_model.keras", monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')

# Train the model
hist = model.fit(
    train_data,
    steps_per_epoch=10,
    epochs=30,
    validation_data=val_data,
    validation_steps=8,
    callbacks=[es, mc]
)

# Load the best saved model (for evaluation or usage)
model = load_model("best_model.keras")

# Print available metrics
h = hist.history
print(h.keys())

!wget https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=0

!unzip train.zip

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from keras.layers import Flatten, Dense
from keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img

from keras.applications.mobilenet import MobileNet, preprocess_input
from keras.losses import categorical_crossentropy

base_model = MobileNet( input_shape=(224,224,3), include_top= False )

for layer in base_model.layers:
  layer.trainable = False


x = Flatten()(base_model.output)
x = Dense(units=7 , activation='softmax' )(x)

# creating our model.
model = Model(base_model.input, x)

model.compile(optimizer='adam', loss= categorical_crossentropy , metrics=['accuracy']  )

train_datagen = ImageDataGenerator(
     zoom_range = 0.2,
     shear_range = 0.2,
     horizontal_flip=True,
     rescale = 1./255
)

train_data = train_datagen.flow_from_directory(directory= "/content/train",
                                               target_size=(224,224),
                                               batch_size=32,
                                  )


train_data.class_indices

val_datagen = ImageDataGenerator(rescale = 1./255 )

# Change the directory to a valid path where your validation data is located
val_data = val_datagen.flow_from_directory(directory= "/content/train",  # or any other existing directory
                                           target_size=(224,224),
                                           batch_size=32,
                                  )

t_img , label = next(train_data) # Changed train_data.next() to next(train_data)

#-----------------------------------------------------------------------------
# function when called will prot the images
def plotImages(img_arr, label):
  """
  input  :- images array
  output :- plots the images
  """
  count = 0
  for im, l in zip(img_arr,label) :
    plt.imshow(im)
    plt.title(im.shape)
    plt.axis = False
    plt.show()

    count += 1
    if count == 10:
      break

#-----------------------------------------------------------------------------
# function call to plot the images
plotImages(t_img, label)

from keras.callbacks import ModelCheckpoint, EarlyStopping

# early stopping
es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')

# model check point
mc = ModelCheckpoint(filepath="best_model.h5", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')

# puting call back in a list
call_back = [es, mc]

hist = model.fit(train_data,  # Use fit instead of fit_generator
                           steps_per_epoch= 10,
                           epochs= 30,
                           validation_data= val_data,
                           validation_steps= 8,
                           callbacks=[es,mc])

from keras.models import load_model
model = load_model("/content/best_model.h5")

h =  hist.history
h.keys()

plt.plot(h['accuracy'])
plt.plot(h['val_accuracy'] , c = "red")
plt.title("acc vs v-acc")
plt.show()

plt.plot(h['loss'])
plt.plot(h['val_loss'] , c = "red")
plt.title("loss vs v-loss")
plt.show()

op = dict(zip( train_data.class_indices.values(), train_data.class_indices.keys()))

path = "/content/angry.jpeg"  # Update to the correct file path
img = load_img(path, target_size=(224,224) )

i = img_to_array(img)/255
input_arr = np.array([i])
input_arr.shape

pred = np.argmax(model.predict(input_arr))

print(f" the image is of {op[pred]}")

# to display the image
plt.imshow(input_arr[0])
plt.title("input image")
plt.show()

import cv2
from deepface import DeepFace
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from PIL import Image
import io

# JS code to capture an image from the webcam
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = '📷 Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});
      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for capture button
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getTracks().forEach(track => track.stop());
      div.remove();

      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename

# Capture photo
filename = take_photo()

# Load image and detect emotion
img = cv2.imread(filename)
result = DeepFace.analyze(img, actions=['emotion'], enforce_detection=False)

# Draw emotion label on image
for face in result:
    x, y, w, h = face["region"]['x'], face["region"]['y'], face["region"]['w'], face["region"]['h']
    emotion = face['dominant_emotion']
    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)
    cv2.putText(img, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

# Show image in Colab
from google.colab.patches import cv2_imshow
cv2_imshow(img)
print("Dominant Emotion:", result[0]['dominant_emotion'])